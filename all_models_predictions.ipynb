{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn import svm, neighbors\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier #RandomizedLasso\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor \n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '~/Documents/GitHub/march-madness/'\n",
    "train = pd.read_csv('train_2010_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels [u'OppT'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7782340159a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m        \u001b[0;34mu'OppNCSOSAdjEM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'OppMooreRank'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'OppW'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'OppL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'OppT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m        u'OppMooreSOS', u'OppMoorePR',]\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtakeout_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtakeout_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels [u'OppT'] not contained in axis"
     ]
    }
   ],
   "source": [
    "## TAKE OUT ALL ADDED DATA \n",
    "takeout_vars = [u'PomeroyRank', u'Conf', u'AdjEM', u'AdjO',\n",
    "       u'AdjD', u'AdjT', u'Luck', u'SOSAdjEM', u'OppO', u'OppD', u'NCSOSAdjEM',\n",
    "       u'MooreRank', u'MooreSOS', u'MoorePR',u'OppPomeroyRank', u'OppConf', u'OppAdjEM', u'OppAdjO', u'OppAdjD',\n",
    "       u'OppAdjT', u'OppLuck', u'OppSOSAdjEM', u'OppOppO', u'OppOppD',\n",
    "       u'OppNCSOSAdjEM', u'OppMooreRank', u'OppW', u'OppL', u'OppT',\n",
    "       u'OppMooreSOS', u'OppMoorePR',]\n",
    "train = train.drop(takeout_vars, axis=1)\n",
    "test = test.drop(takeout_vars, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fragile families open source code \n",
    "def factorize(df):\n",
    "    \"\"\"Convert features of type 'object', e.g. 'string', to categorical\n",
    "    variables or factors.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df.loc[:,col].dtype == object:\n",
    "            factors, values = pd.factorize(df[col])\n",
    "            df.loc[:,col] = factors\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DROP THE WLT COLUMNS TO AVOID THE ISSUE RN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = factorize(train)\n",
    "test = factorize(test)\n",
    "train = train.drop(['W', 'L', 'T'], axis=1)\n",
    "test = test.drop(['W', 'L', 'T'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Outcome']\n",
    "X_train = train.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['Outcome']\n",
    "X_test = test.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(cm, y_labels, cmap=plt.cm.Blues, filename='untitled.png'):\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_test(x_data, x_labels, y_data, y_labels):\n",
    "    starttime = time.time()\n",
    "\n",
    "    # binary models\n",
    "    models = [\"BNB\", \"GNB\", \"LDA\",\"SVM_L\", \"SVM_G\", \"5NN\", \"LR2\", \"P2\", \"SGD\",\"ADA\", \"DT\", \"RF\", \"DPGMM\", \"ET\", \"GMM\", \"MLP\"] #\"SVM_L\", \"SVM_G\", \"P2\", \"DT\",  \"ADA_R\", \n",
    "    clfs = [BernoulliNB(), \\\n",
    "            GaussianNB(), \\\n",
    "            LinearDiscriminantAnalysis(), \\\n",
    "            svm.SVC(kernel = 'linear', probability=True), \\\n",
    "            svm.SVC(kernel='rbf', probability=True), \\\n",
    "            neighbors.KNeighborsClassifier(n_neighbors=5), \\\n",
    "            LogisticRegression(), \\\n",
    "            Perceptron(penalty='l2',tol=None,max_iter=1000), \\\n",
    "            SGDClassifier(tol=0.0001, power_t=0.4, average=True), \\\n",
    "            AdaBoostClassifier(base_estimator=None, n_estimators=100), \\\n",
    "            DecisionTreeClassifier(), \\\n",
    "            RandomForestClassifier(oob_score=True),  \\\n",
    "            BayesianGaussianMixture(n_components=2,max_iter=1000, weight_concentration_prior_type='dirichlet_process', tol=0.0001), \\\n",
    "            ExtraTreesClassifier(bootstrap=True, oob_score=True, n_estimators=4), \\\n",
    "            GaussianMixture(n_components=2, tol=0.0001, max_iter=1000, n_init=2), \\\n",
    "            MLPClassifier(activation='relu', alpha=0.00001, max_iter=1000)]\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(clfs)):\n",
    "        print \"model being tested: {0}\".format(models[i])\n",
    "        time_start = time.time()\n",
    "        clf = clfs[i].fit(x_data, x_labels)\n",
    "        predict = clf.predict(y_data)\n",
    "        runtime = time.time() - time_start\n",
    "        p = metrics.precision_score(y_labels, predict)\n",
    "        r = metrics.recall_score(y_labels, predict, average=\"macro\")\n",
    "        f = metrics.f1_score(y_labels, predict)\n",
    "\n",
    "        # find outliers\n",
    "        # data = [('challengeID', y_data['challengeID'].values),\n",
    "        #         ('predicted', predict),\n",
    "        #         ('label', y_labels.values)]\n",
    "        # labels_and_predicted = pd.DataFrame.from_items(data)\n",
    "        # outliers = y_data.merge(labels_and_predicted, on='challengeID')\n",
    "        # outliers = outliers[outliers['label'] != outliers['predicted']]\n",
    "\n",
    "        # num_mislabeled = outliers.shape[0]\n",
    "        # a_new = -1\n",
    "        # p_new = -1\n",
    "        # r_new = -1\n",
    "        # f_new = -1\n",
    "\n",
    "        # if (outliers['label'].unique().size > 1):\n",
    "        #     # train separate model on outliers\n",
    "        #     mislabeled_labels = outliers['label']\n",
    "        #     mislabeled_samples = outliers.drop(['label', 'predicted'], axis=1)\n",
    "\n",
    "        #     (train_vars,validate_vars,train_outcomes,validate_outcomes) = train_test_split(mislabeled_samples,mislabeled_labels,test_size=0.2)\n",
    "\n",
    "        #     clf_new = clfs[i].fit(train_vars, train_outcomes)\n",
    "        #     validate_predicted = clf_new.predict(validate_vars)\n",
    "\n",
    "        #     # evaluate\n",
    "        #     a_new = metrics.accuracy_score(validate_outcomes, validate_predicted)\n",
    "        #     p_new = metrics.precision_score(validate_outcomes, validate_predicted)\n",
    "        #     r_new = metrics.recall_score(validate_outcomes, validate_predicted, average=\"macro\")\n",
    "        #     f_new = metrics.f1_score(validate_outcomes, validate_predicted)\n",
    "\n",
    "        # results.append([models[i], a, p, r, f, runtime, num_mislabeled, a_new, p_new, r_new, f_new])\n",
    "        results.append([models[i], p, r, f, runtime])\n",
    "        # create confusion matrix \n",
    "        cm = metrics.confusion_matrix(y_labels, predict)\n",
    "        plot_confusion(cm, y_labels, filename='{0}_confusion.png'.format(models[i]))\n",
    "    print tabulate.tabulate(results, headers=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','Runtime'])\n",
    "    print \"Binary test took {0} secs\".format(time.time() - starttime)\n",
    "    return pd.DataFrame(data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results = bin_test(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results.to_csv('~/Documents/GitHub/march-madness/prediction_results')\n",
    "prediction_results = np.round(prediction_results, decimals=2)\n",
    "print prediction_results.to_latex(index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only 2018 data first, for test set \n",
    "test2018 = test[test['Season']==2018]\n",
    "# get only round one data \n",
    "test2018 = test2018[test2018['Round']==1]\n",
    "\n",
    "test2018_y = test2018['Outcome']\n",
    "test2018_X = test2018.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(test2018_X)\n",
    "a = metrics.accuracy_score(test2018_y, predict)\n",
    "p = metrics.precision_score(test2018_y, predict)\n",
    "r = metrics.recall_score(test2018_y, predict, average=\"macro\")\n",
    "f = metrics.f1_score(test2018_y, predict)\n",
    "result = [a, p, r, f]\n",
    "print result\n",
    "weights = clf.coef_\n",
    "idx = np.argsort(weights)\n",
    "weights = weights[0,idx]\n",
    "features = np.asarray(X_test.columns)\n",
    "features = features[idx]\n",
    "weights = pd.DataFrame(data=weights)\n",
    "features = pd.DataFrame(data=features)\n",
    "FW = pd.concat([features, weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal LR \n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "a = metrics.accuracy_score(y_test, predict)\n",
    "p = metrics.precision_score(y_test, predict)\n",
    "r = metrics.recall_score(y_test, predict, average=\"macro\")\n",
    "f = metrics.f1_score(y_test, predict)\n",
    "result = [a, p, r, f]\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(bootstrap=True, oob_score=True, n_estimators=4)\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "a = metrics.accuracy_score(y_test, predict)\n",
    "p = metrics.precision_score(y_test, predict)\n",
    "r = metrics.recall_score(y_test, predict, average=\"macro\")\n",
    "f = metrics.f1_score(y_test, predict)\n",
    "result = [a, p, r, f]\n",
    "print result\n",
    "weights = clf.feature_importances_\n",
    "idx = np.argsort(weights)\n",
    "weights = weights[idx]\n",
    "features = np.asarray(X_test.columns)\n",
    "features = features[idx]\n",
    "weights = pd.DataFrame(data=weights)\n",
    "features = pd.DataFrame(data=features)\n",
    "FW = pd.concat([features, weights], axis=1)\n",
    "FW.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "a = metrics.accuracy_score(y_test, predict)\n",
    "p = metrics.precision_score(y_test, predict)\n",
    "r = metrics.recall_score(y_test, predict, average=\"macro\")\n",
    "f = metrics.f1_score(y_test, predict)\n",
    "result = [a, p, r, f]\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = clf.coef_\n",
    "idx = np.argsort(weights)\n",
    "weights = weights[0,idx]\n",
    "features = np.asarray(X_test.columns)\n",
    "features = features[idx]\n",
    "weights = pd.DataFrame(data=weights)\n",
    "features = pd.DataFrame(data=features)\n",
    "FW = pd.concat([features, weights])\n",
    "FW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "DPGMM\n",
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GMM\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "QuadraticDiscriminantAnalysis\n",
      "RandomForestClassifier\n",
      "SGDClassifier\n",
      "SVC\n",
      "VBGMM\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n",
      "_DPGMMBase\n",
      "_GMMBase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(tol=0.0001, power_t=0.4, average=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "p = metrics.precision_score(y_test, predict)\n",
    "r = metrics.recall_score(y_test, predict, average=\"macro\")\n",
    "f = metrics.f1_score(y_test, predict)\n",
    "result = [p, r, f]\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
